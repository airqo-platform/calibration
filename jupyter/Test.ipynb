{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from calibration import placeinducingpoints, CalibrationSystem, SparseModel, Kernel\n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Only Demo\n",
    "\n",
    "Here we have four sensors, their calibration remains constant (although the model doesn't know that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 501\n",
    "X = np.linspace(0,260,N)[:,None]\n",
    "X = np.c_[X,np.zeros(N),np.ones(N)]\n",
    "Y = np.c_[np.linspace(20,40,N)[:,None],np.linspace(40,80,N)[:,None]]\n",
    "Y[:,1]*=np.cos(X[:,0]/10)*0.2+1\n",
    "Y[50:70,1]+=np.random.randn(20)*np.linspace(0,10,20)\n",
    "Y[70:90,1]+=np.random.randn(20)*np.linspace(10,0,20)\n",
    "Y[200:250,1]+=np.random.randn(50)*np.linspace(0,30,50)\n",
    "Y[250:300,1]+=np.random.randn(50)*np.linspace(30,0,50)\n",
    "Y[:,1]+=np.random.randn(N)*np.linspace(3,0,N)\n",
    "refsensor = np.array([1,0])\n",
    "Z = np.linspace(0,260,50)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "psd_kernels = tfp.math.psd_kernels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fn(samps,Y):\n",
    "    return Y*(1+samps[:,:,0:1])\n",
    "\n",
    "count=0\n",
    "class NewCalSys(CalibrationSystem):\n",
    "    def __init__(self,X,Y,Z,refsensor,C,transform_fn,gpflowkernel,likemodel='fixed',gpflowkernellike=None,likelihoodstd=1.0,jitter=1e-3):\n",
    "        \"\"\"\n",
    "        likemodel specifies how the likelihood is modelled, it can be one of three values:\n",
    "          - fixed [default, uses the value in likelihoodstd]\n",
    "          - single [optimise a single value [TODO Not Implemented]]\n",
    "          - distribution [uses gpflowkernellike]\n",
    "          - process [uses gpflowkernellike]\n",
    "        \"\"\"\n",
    "        self.likemodel = likemodel\n",
    "        S = len(refsensor)\n",
    "        self.C = C\n",
    "        self.Y = Y        \n",
    "        self.k = Kernel(gpflowkernel)\n",
    "        \n",
    "        self.likelihoodstd = likelihoodstd\n",
    "        self.X = np.c_[np.tile(np.r_[np.c_[X[:,0],X[:,1]],np.c_[X[:,0],X[:,2]]],[self.C,1]),np.repeat(np.arange(self.C),2*len(X))]        \n",
    "        \n",
    "        if Z.shape[1]==1:\n",
    "            Ztemp = np.c_[np.tile(Z,[S,1]),np.repeat(np.arange(S),len(Z))]\n",
    "        if Z.shape[1]==2:\n",
    "            Ztemp = Z\n",
    "        self.Z = np.c_[np.tile(Ztemp,[C,1]),np.repeat(np.arange(self.C),len(Ztemp))]        \n",
    "        \n",
    "        \n",
    "        if likemodel=='distribution' or likemodel=='process':\n",
    "            assert gpflowkernellike is not None, \"You need to specify the kernel to use a distribution or process\"\n",
    "            self.klike = Kernel(gpflowkernellike)\n",
    "            self.Xlike = np.c_[np.r_[np.c_[X[:,0],X[:,1]],np.c_[X[:,0],X[:,2]]],np.repeat(0,2*len(X))]\n",
    "            self.Zlike = np.c_[Ztemp,np.repeat(0,len(Ztemp))]\n",
    "            \n",
    "        self.N = N = len(X)\n",
    "        self.refsensor = refsensor.astype(np.float32)\n",
    "        self.jitter = jitter\n",
    "        self.transform_fn = transform_fn        \n",
    "        self.precompute()\n",
    "        \n",
    "    def precompute(self):    \n",
    "        #definition of q(u)\n",
    "        M = self.Z.shape[0]\n",
    "        self.mu = tf.Variable(0.001*tf.random.normal([M,1]))\n",
    "        self.scale = tf.Variable(0.1*np.tril(0.1*np.random.randn(M,M)+0.1*np.eye(M)),dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.likemodel=='distribution' or self.likemodel=='process':\n",
    "            Mlike = self.Zlike.shape[0]\n",
    "            self.mulike = tf.Variable(0.0001*tf.random.normal([Mlike,1]))\n",
    "            mu_u = tf.Variable(np.full([Mlike],-12),dtype=tf.float32)\n",
    "            cov_u = tf.Variable(self.klike.matrix(self.Zlike,self.Zlike),dtype=tf.float32)\n",
    "            self.pulike = tfd.MultivariateNormalFullCovariance(mu_u,cov_u+np.eye(cov_u.shape[0])*self.jitter)\n",
    "            self.likeoptimizer = tf.keras.optimizers.Adam(learning_rate=0.08,amsgrad=False) #0.8\n",
    "            self.smlike = SparseModel(self.Xlike,self.Zlike,1,self.k)\n",
    "        else:\n",
    "            self.mulike = None\n",
    "        if self.likemodel=='process':\n",
    "            self.scalelike = tf.Variable(1e-10*np.eye(Mlike),dtype=tf.float32)\n",
    "        else:\n",
    "            self.scalelike = None\n",
    "        \n",
    "        #parameters for p(u)\n",
    "        mu_u = tf.zeros([M],dtype=tf.float32)\n",
    "        cov_u = tf.Variable(self.k.matrix(self.Z,self.Z),dtype=tf.float32)\n",
    "        self.pu = tfd.MultivariateNormalFullCovariance(mu_u,cov_u+np.eye(cov_u.shape[0])*self.jitter)\n",
    "        self.ref = tf.gather(self.refsensor,tf.transpose(tf.reshape(self.X[:(2*self.N),1:2].astype(int),[2,self.N])))\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.02,amsgrad=False) #0.2\n",
    "        self.sm = SparseModel(self.X,self.Z,self.C,self.k)\n",
    "        \n",
    "    def likelihoodfn_nonstationary(self,scaledA,scaledB,varparamA,varparamB):\n",
    "        return tfd.Normal(0,0.00001+tf.sqrt(tf.exp(varparamA)+tf.exp(varparamB))).log_prob(scaledA-scaledB)    \n",
    "    \n",
    "    def likelihoodfn(self,scaledA,scaledB):\n",
    "        return tfd.Normal(0,self.likelihoodstd).log_prob(scaledA-scaledB)\n",
    "        \n",
    "    #@tf.function\n",
    "    def run(self,its=None,samples=100,threshold=1.0):\n",
    "        \"\"\"Set its to None to automatically stop\n",
    "        when the ELBO has reduced by less than threshold\n",
    "        (between rolling averages of the last 50 calculations\n",
    "        and the 50 before that)\"\"\"\n",
    "        elbo_record = []\n",
    "        it = 0\n",
    "        while (its is None) or (it<its):\n",
    "            it+=1\n",
    "            with tf.GradientTape() as tape:\n",
    "                qu = tfd.MultivariateNormalTriL(self.mu[:,0],self.scale)\n",
    "                samps = self.sm.get_samples(self.mu,self.scale,samples)\n",
    "                scaled = tf.concat([self.transform_fn(samps[:,:,::2],self.Y[:,0:1]),self.transform_fn(samps[:,:,1::2],self.Y[:,1:2])],2)\n",
    "                scaled = (scaled * (1-self.ref)) + (self.Y * self.ref)\n",
    "                \n",
    "                if self.mulike is not None: #if we have non-stationary likelihood variance...\n",
    "                    qulike = tfd.MultivariateNormalTriL(self.mulike[:,0],self.scalelike)              \n",
    "                    like = self.smlike.get_samples(self.mulike,self.scalelike,samples)\n",
    "                    ell = tf.reduce_mean(tf.reduce_sum(self.likelihoodfn_nonstationary(scaled[:,:,0],scaled[:,:,1],like[:,:,0]*(1-self.ref[:,0])-1000*self.ref[:,0],like[:,:,1]*(1-self.ref[:,1])-1000*self.ref[:,1]),1))\n",
    "                else: #stationary likelihood variance\n",
    "                    ell = tf.reduce_mean(tf.reduce_sum(self.likelihoodfn(scaled[:,:,0],scaled[:,:,1]),1))\n",
    "                \n",
    "                elbo_loss = -ell+tfd.kl_divergence(qu,self.pu)\n",
    "                \n",
    "                if self.likemodel=='process':\n",
    "                    assert self.mulike is not None\n",
    "                    assert self.scalelike is not None\n",
    "                    elbo_loss += tfd.kl_divergence(qulike,self.pulike)\n",
    "                if self.likemodel=='distribution':\n",
    "                    assert self.mulike is not None\n",
    "                    elbo_loss -= self.pulike.log_prob(self.mulike[:,0])\n",
    "\n",
    "                if it%100==0: print(\"%d (ELBO=%0.4f)\" % (it, elbo_loss))\n",
    "                \n",
    "                if (self.mulike is None) or (it%50<25): #optimise latent fns\n",
    "                    gradients = tape.gradient(elbo_loss, [self.mu,self.scale])\n",
    "                    self.optimizer.apply_gradients(zip(gradients, [self.mu, self.scale]))  \n",
    "                else: #this optimises the likelihood...\n",
    "                    if self.likemodel=='distribution':\n",
    "                        gradients = tape.gradient(elbo_loss, [self.mulike])\n",
    "                        self.likeoptimizer.apply_gradients(zip(gradients, [self.mulike]))\n",
    "                    if self.likemodel=='process':\n",
    "                        gradients = tape.gradient(elbo_loss, [self.mulike,self.scalelike])\n",
    "                        self.likeoptimizer.apply_gradients(zip(gradients, [self.mulike,self.scalelike]))\n",
    "\n",
    "                elbo_record.append(elbo_loss)\n",
    "            if its is None:\n",
    "                if it>100:\n",
    "                    oldm = np.mean(elbo_record[-100:-50])\n",
    "                    m = np.mean(elbo_record[-50:])\n",
    "                    if np.abs(oldm-m)<threshold:\n",
    "                        break\n",
    "        return np.array(elbo_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bob Turner\\.conda\\envs\\calibration\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:334: MultivariateNormalFullCovariance.__init__ (from tensorflow_probability.python.distributions.mvn_full_covariance) is deprecated and will be removed after 2019-12-01.\n",
      "Instructions for updating:\n",
      "`MultivariateNormalFullCovariance` is deprecated, use `MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))` instead.\n",
      "100 (ELBO=3554.0225)\n",
      "200 (ELBO=2495.2034)\n",
      "300 (ELBO=2196.8464)\n",
      "400 (ELBO=2054.9392)\n",
      "500 (ELBO=1987.6077)\n",
      "600 (ELBO=1953.8877)\n",
      "700 (ELBO=1922.5397)\n",
      "800 (ELBO=1907.9066)\n",
      "900 (ELBO=1895.7302)\n"
     ]
    }
   ],
   "source": [
    "k = gpflow.kernels.RBF(100,20)+gpflow.kernels.Bias(100)\n",
    "klike = gpflow.kernels.RBF(100,20)+gpflow.kernels.Bias(100)\n",
    "cs = NewCalSys(X, Y, Z, refsensor, 1, transform_fn, k, likemodel='process',gpflowkernellike=klike,likelihoodstd=0.5)\n",
    "import time\n",
    "before = time.time()\n",
    "elbo_record = cs.run()\n",
    "print(time.time()-before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elbo_record)\n",
    "plt.grid()\n",
    "#plt.yscale('log')\n",
    "plt.ylim([np.min(elbo_record)*0.95,np.min(elbo_record)*1.05])\n",
    "plt.hlines(np.mean(elbo_record[-100:]),0,2100,'k')\n",
    "plt.hlines(np.mean(elbo_record[-200:-100]),0,2100,'b')\n",
    "plt.hlines(np.mean(elbo_record[-300:-200]),0,2100,'g')\n",
    "plt.hlines(np.mean(elbo_record[-400:-300]),0,2100,'r')\n",
    "plt.hlines(np.mean(elbo_record[-500:-400]),0,2100,'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build test matrix\n",
    "C = 1\n",
    "for si,refs in enumerate(refsensor):\n",
    "    if refs: continue\n",
    "    x = np.linspace(0,260,151)\n",
    "    testX = np.zeros([0,3])\n",
    "    for ci in range(C):\n",
    "        tempX = np.c_[x,np.ones_like(x)*si,np.full_like(x,ci)]\n",
    "        testX = np.r_[testX,tempX]#.astype(int)\n",
    "    testsm = SparseModel(testX,cs.Z,C,Kernel(k))\n",
    "    qf_mu,qf_cov = testsm.get_qf(cs.mu,cs.scale)\n",
    "    if cs.mulike is not None:\n",
    "        qf_mulike,qf_covlike = testsm.get_qf(cs.mulike,cs.scalelike)\n",
    "        sampslike = testsm.get_samples_one_sensor(cs.mulike,cs.scalelike)\n",
    "    samps = testsm.get_samples_one_sensor(cs.mu,cs.scale)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=[18,12])\n",
    "    plt.plot(x,(samps[::,:,0].numpy().T),'k-',alpha=0.1);\n",
    "    #plt.plot(-getsensorshift(x,np.ones_like(x).astype(int)))\n",
    "    plt.grid()\n",
    "    plt.vlines(Z[:,0],0,.1)\n",
    "    #plt.ylim([-.2,1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('standard deviation estimate of likelihood')\n",
    "plt.plot(x,np.sqrt(np.exp(sampslike[:,:,0].numpy().T)),'b.',alpha=0.1);\n",
    "plt.gca().set_yscale('log')\n",
    "#plt.ylim([1e-3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qf_mulike.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14,7])\n",
    "plt.plot(x,1/(1+qf_mu[:151,0]),'k-')\n",
    "#plt.plot(x,1/(1+qf_mu[:151,0])+2*np.sqrt(np.diag(qf_cov)[:151])+2*np.sqrt(np.exp(qf_mulike[:151,0]))/np.mean(Y[:,0]),'k--',alpha=0.5)\n",
    "#plt.plot(x,1/(1+qf_mu[:151,0])-2*np.sqrt(np.diag(qf_cov)[:151])-2*np.sqrt(np.exp(qf_mulike[:151,0]))/np.mean(Y[:,0]),'k--',alpha=0.5)\n",
    "plt.plot(x,1/(1+(qf_mu[:151,0]+2*np.sqrt(np.diag(qf_cov)[:151]))+2*np.sqrt(np.exp(qf_mulike[:151,0]))/np.mean(Y[:,0])),'k--',alpha=0.5)\n",
    "plt.plot(x,1/(1+(qf_mu[:151,0]-2*np.sqrt(np.diag(qf_cov)[:151]))),'k-',alpha=0.5)\n",
    "plt.plot(x,1/(1+(qf_mu[:151,0]+2*np.sqrt(np.diag(qf_cov)[:151]))),'k-',alpha=0.5)\n",
    "\n",
    "plt.plot(X[:,0],Y[:,1]/Y[:,0],'x')\n",
    "plt.ylim([0,4])\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(qf_cov)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,np.sqrt(np.diag(qf_cov)[:151]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sqrt(np.exp(qf_mu[151:,0])))\n",
    "#plt.ylim([0,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear transform demo\n",
    "\n",
    "Just two sensors, the low-cost sensor has a drift in which its offset increases over time.\n",
    "\n",
    "There is obviously some ambiguity from the model's point of view as to whether the difference between the sensors is due scaling or offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.linspace(0,20,21),np.zeros(21),np.ones(21)]\n",
    "y = np.cos(X[:,0])*25+100\n",
    "Y = np.c_[y,y+X[:,0]]\n",
    "refsensor = np.array([1,0])\n",
    "Z = np.linspace(0,20,5)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def transform_fn(samps,Y):\n",
    "#    return 100*(samps[:,:,0:1]) + Y*samps[:,:,1:2] + (samps[:,:,2:3]*Y**2)/1000\n",
    "\n",
    "def transform_fn(samps,Y):\n",
    "    return 100*samps[:,:,0:1] + Y*samps[:,:,1:2]\n",
    "\n",
    "k = gpflow.kernels.RBF(1,15)+gpflow.kernels.Bias(1)\n",
    "cs = CalibrationSystem(X, Y, Z, refsensor, 2, transform_fn, k,likelihoodstd=0.05)\n",
    "cs.run(its=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build test matrix\n",
    "C = 2\n",
    "for si,refs in enumerate(refsensor):\n",
    "    if refs: continue\n",
    "    x = np.linspace(0,15,151)\n",
    "    testX = np.zeros([0,3])\n",
    "    for ci in range(C):\n",
    "        tempX = np.c_[x,np.ones_like(x)*si,np.full_like(x,ci)]\n",
    "        testX = np.r_[testX,tempX]#.astype(int)\n",
    "    testsm = SparseModel(testX,cs.Z,C,Kernel(k))\n",
    "    qf_mu,qf_cov = testsm.get_qf(cs.mu,cs.scale)\n",
    "    samps = testsm.get_samples_one_sensor(cs.mu,cs.scale)\n",
    "\n",
    "    plt.figure(figsize=[6,4])\n",
    "    plt.plot(x,samps[:,:,1].numpy().T,'k.',alpha=0.01);\n",
    "    plt.plot(x,100*samps[:,:,0].numpy().T,'b.',alpha=0.01);\n",
    "    #plt.plot(-getsensorshift(x,np.ones_like(x).astype(int)))\n",
    "    plt.grid()\n",
    "    plt.vlines(Z[:,0],-.3,-.2)\n",
    "    plt.ylim([-20,2])\n",
    "    plt.xlim([0,15])\n",
    "    plt.hlines(1,0,15,'k',label='scale')\n",
    "    plt.plot([0,15],[0,-15],'b-',label='offset')\n",
    "    plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
